This exercise focuses on building a multi-modal semantic search engine using the CLIP ViT-B/32 model for both text and image embeddings, which are stored in a Chroma collection. Users can input queries to retrieve relevant artifacts from a given PDF file.
